import re
from collections import Counter
import pymorphy3
import ollama
from datetime import datetime
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import numpy as np
import os

# ======================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–û–ù–ù–´–ï –ù–ê–°–¢–†–û–ô–ö–ò
# ======================

# –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤
USER_ALIAS = '–Ø'              # –í–∞—à–µ –∏–º—è –≤ –ø–µ—Ä–µ–ø–∏—Å–∫–µ
FRIEND_ALIAS = '–î—Ä—É–≥'         # –ò–º—è –¥—Ä—É–≥–∞ –≤ –ø–µ—Ä–µ–ø–∏—Å–∫–µ

FILE_TAG = 'friend'

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Ñ–∞–π–ª–æ–≤
INPUT_FILE = f'{FILE_TAG}_chat_history.txt'             # –§–∞–π–ª —Å –ø–µ—Ä–µ–ø–∏—Å–∫–æ–π
OUTPUT_REPORT = f'{FILE_TAG}_chat_analysis_report.txt'  # –§–∞–π–ª –æ—Ç—á–µ—Ç–∞
WORDCLOUD_FILE = f'{FILE_TAG}_wordcloud.png'            # –û–±–ª–∞–∫–æ —Å–ª–æ–≤
LENGTH_PLOT_FILE = f'{FILE_TAG}_message_lengths.png'    # –ì—Ä–∞—Ñ–∏–∫ –¥–ª–∏–Ω—ã —Å–æ–æ–±—â–µ–Ω–∏–π
POS_PLOT_FILE = f'{FILE_TAG}_pos_distribution.png'      # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∞—Å—Ç–µ–π —Ä–µ—á–∏

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∞–Ω–∞–ª–∏–∑–∞
OLLAMA_MODEL = 'deepseek-r1'
MIN_WORD_LENGTH = 1           # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞
TOP_WORDS_LIMIT = 10          # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø–æ–≤—ã—Ö —Å–ª–æ–≤
MESSAGE_SAMPLE_SIZE = 500     # –°–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞

# –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —á–∞—Å—Ç–µ–π —Ä–µ—á–∏ (pymorphy3 tags)
FILTER_POS_TAGS = {
    'PREP',  # –ü—Ä–µ–¥–ª–æ–≥–∏
    'CONJ',  # –°–æ—é–∑—ã
    'PRCL',  # –ß–∞—Å—Ç–∏—Ü—ã
    'INTJ',  # –ú–µ–∂–¥–æ–º–µ—Ç–∏—è
    'NPRO',  # –ú–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è
    'NUMR',  # –ß–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ
    'PRED',  # –ü—Ä–µ–¥–∏–∫–∞—Ç–∏–≤—ã
}

# –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
CUSTOM_STOPWORDS = {
    '—ç—Ç–æ', '–≤–æ—Ç', '–Ω—É', '–¥–∞', '–Ω–µ—Ç', '–Ω–µ', '–ª–∏', '–∂–µ', '–±—ã', '—Ç–æ',
    '–∫–∞–∫', '—Ç–∞–∫', '—É–∂–µ', '–µ—â–µ', '–∏–ª–∏', '–Ω–æ', '–∑–∞', '–∏–∑', '–æ—Ç', '–¥–æ',
    '–∏', '—É', '–Ω–∞', '—Ç—ã', '—è', '—á—Ç–æ', '—Ç–∞–º', '–µ—Å–ª–∏', '–≤', '–≤—Å–µ',
    '–∞', '–ø–æ', '—Å', '–º–Ω–µ', '–µ—Å–ª–∏', '–µ—Å—Ç—å', '–¥–ª—è', '–∫–∞–∫–æ–π',
    '—Ç–∞–∫–æ–π', '–≤–µ—Å—å', '–∫–æ—Ç–æ—Ä—ã–π', '–±—ã—Ç—å', 'http', 'ru', 'com', 'pikabu',
    'utm_medium', '>youtu', 'story', 't', 'https', 'utm_source', '—ç—Ç–æ—Ç',
    '—Ç—É—Ç', 'www', '–æ–±', '—Ç–æ—Ç'
}

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
PLOT_COLORS = {
    'user': '#4e79a7',
    'friend': '#e15759',
    'other': '#59a14f'
}
FONT_PATH = '/System/Library/Fonts/Supplemental/Arial Unicode.ttf'
plt.style.use('ggplot')

# ======================
# –ö–õ–ê–°–° –ê–ù–ê–õ–ò–ó–ê –ü–ï–†–ï–ü–ò–°–ö–ò
# ======================
class ChatAnalyzer:
    def __init__(self):
        self.morph = pymorphy3.MorphAnalyzer()
        self.messages = []
        self.stopwords = self._init_stopwords()
        self.pos_mapping = {
            'NOUN': '–°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ',
            'VERB': '–ì–ª–∞–≥–æ–ª—ã',
            'ADJF': '–ü—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ',
            'ADVB': '–ù–∞—Ä–µ—á–∏—è',
            'INFN': '–ò–Ω—Ñ–∏–Ω–∏—Ç–∏–≤—ã',
            'GRND': '–î–µ–µ–ø—Ä–∏—á–∞—Å—Ç–∏—è',
            'PRTF': '–ü—Ä–∏—á–∞—Å—Ç–∏—è'
        }

    def _init_stopwords(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–æ–ø-—Å–ª–æ–≤ —Å —É—á–µ—Ç–æ–º –≤—Å–µ—Ö —Å–ª–æ–≤–æ—Ñ–æ—Ä–º"""
        base_words = set(CUSTOM_STOPWORDS)
        
        for word in CUSTOM_STOPWORDS:
            try:
                parsed = self.morph.parse(word)[0]
                base_words.update([f.word for f in parsed.lexeme])
            except:
                continue
                
        return base_words

    def _filter_word(self, word):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å–ª–æ–≤–∞"""
        if len(word) < MIN_WORD_LENGTH:
            return False
            
        parsed = self.morph.parse(word)[0]
        pos = str(parsed.tag).split(',')[0]
        
        if pos in FILTER_POS_TAGS:
            return False
            
        if parsed.normal_form in self.stopwords:
            return False
            
        return True

    def load_chat(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–∞—Ä—Å–∏–Ω–≥ –ø–µ—Ä–µ–ø–∏—Å–∫–∏ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
        if not os.path.exists(INPUT_FILE):
            raise FileNotFoundError(f"–§–∞–π–ª –ø–µ—Ä–µ–ø–∏—Å–∫–∏ {INPUT_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        with open(INPUT_FILE, 'r', encoding='utf-8') as f:
            for line in f:                    
                if match := re.match(rf'^({USER_ALIAS}|{FRIEND_ALIAS}):\s*(.+)', line.strip()):
                    self.messages.append({
                        'sender': match.group(1),
                        'text': match.group(2),
                        'words': [ word if word != '–º–æ—á—å' else '–º–æ–∂–Ω–æ' for word in self._process_text(match.group(2)) ]
                    })

    def _process_text(self, text):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º –∞–Ω–∞–ª–∏–∑–æ–º"""
        words = re.findall(rf'\b[–∞-—è—ë]{{{MIN_WORD_LENGTH},}}\b', text.lower())
        return [
            self.morph.parse(w)[0].normal_form 
            for w in words 
            if self._filter_word(w)
        ]

    def basic_stats(self):
        """–ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–µ—Ä–µ–ø–∏—Å–∫–∏"""
        stats = {
            'total': len(self.messages),
            'user': sum(1 for m in self.messages if m['sender'] == USER_ALIAS),
            'message_lengths': {
                'user': [len(m['text']) for m in self.messages if m['sender'] == USER_ALIAS],
                'friend': [len(m['text']) for m in self.messages if m['sender'] == FRIEND_ALIAS]
            },
            'word_counts': {
                'user': sum(len(m['words']) for m in self.messages if m['sender'] == USER_ALIAS),
                'friend': sum(len(m['words']) for m in self.messages if m['sender'] == FRIEND_ALIAS)
            }
        }
        
        stats.update({
            'user_ratio': stats['user'] / stats['total'],
            'avg_length': {
                'user': np.mean(stats['message_lengths']['user']) if stats['message_lengths']['user'] else 0,
                'friend': np.mean(stats['message_lengths']['friend']) if stats['message_lengths']['friend'] else 0
            },
            'avg_words': {
                'user': stats['word_counts']['user'] / stats['user'] if stats['user'] else 0,
                'friend': stats['word_counts']['friend'] / (stats['total'] - stats['user']) if stats['total'] != stats['user'] else 0
            }
        })
        
        return stats

    def vocabulary_analysis(self):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–ª–æ–≤–∞—Ä–Ω–æ–≥–æ –∑–∞–ø–∞—Å–∞"""
        def analyze(words):
            cnt = Counter(words)
            unique = set(words)
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —á–∞—Å—Ç—è–º —Ä–µ—á–∏
            pos_stats = Counter()
            for word in words:
                parsed = self.morph.parse(word)[0]
                main_pos = str(parsed.tag).split(',')[0]
                pos_stats[main_pos] += 1
            
            return {
                'total': len(words),
                'unique': len(unique),
                'diversity': len(unique) / len(words) if words else 0,
                'top': cnt.most_common(TOP_WORDS_LIMIT),
                'pos_stats': pos_stats.most_common(10)
            }

        words = {
            'user': [w for m in self.messages if m['sender'] == USER_ALIAS for w in m['words']],
            'friend': [w for m in self.messages if m['sender'] == FRIEND_ALIAS for w in m['words']],
            'all': [w for m in self.messages for w in m['words']]
        }

        return {
            'user': analyze(words['user']),
            'friend': analyze(words['friend']),
            'all': analyze(words['all'])
        }

    def emotional_analysis(self):
        """–ê–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–æ–Ω–∞ —á–µ—Ä–µ–∑ LLM"""
        sample = '\n'.join(
            f"{m['sender']}: {m['text']}" 
            for m in self.messages[-MESSAGE_SAMPLE_SIZE:]
        )
        
        prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ç–æ–Ω –≤ —ç—Ç–æ–º –¥–∏–∞–ª–æ–≥–µ:
{sample}

–û—Ç–≤–µ—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ:
- –û—Å–Ω–æ–≤–Ω—ã–µ —ç–º–æ—Ü–∏–∏: 
- –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —ç–º–æ—Ü–∏–π: 
- –û–±—â–∏–π —Ç–æ–Ω (–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π/–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π/–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π): 
- –ò–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å (1-10): """
        
        try:
            response = ollama.generate(
                model=OLLAMA_MODEL,
                prompt=prompt,
                options={'temperature': 0.4}
            )
            return response['response']
        except Exception as e:
            return f"‚ö† –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —ç–º–æ—Ü–∏–π: {str(e)}"

    def generate_wordcloud(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±–ª–∞–∫–∞ —Å–ª–æ–≤ (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
        text = ' '.join(' '.join(m['words']) for m in self.messages)
        
        wordcloud = WordCloud(
            width=2560,
            height=1440,
            background_color='white',
            colormap='viridis',
            font_path=FONT_PATH,
            stopwords=self.stopwords,
            max_words=300,
            collocations=False
        ).generate(text)
        
        plt.figure(figsize=(20, 10))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis('off')
        plt.savefig(WORDCLOUD_FILE, bbox_inches='tight')
        plt.close()

    def plot_message_lengths(self):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–ª–∏–Ω—ã —Å–æ–æ–±—â–µ–Ω–∏–π (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è)"""
        stats = self.basic_stats()
        
        plt.figure(figsize=(12, 6))
        bins = np.linspace(0, 200, 20)
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã
        plt.hist(
            [stats['message_lengths']['user'], stats['message_lengths']['friend']],
            bins=bins,
            label=[USER_ALIAS, FRIEND_ALIAS],
            color=[PLOT_COLORS['user'], PLOT_COLORS['friend']],
            alpha=0.6
        )
        
        plt.xlabel('–î–ª–∏–Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏—è (—Å–∏–º–≤–æ–ª–æ–≤)', fontsize=12)
        plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π', fontsize=12)
        plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω—ã —Å–æ–æ–±—â–µ–Ω–∏–π', fontsize=14)
        plt.legend(fontsize=12)
        plt.grid(True, linestyle='--', alpha=0.6)
        
        # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        plt.savefig(LENGTH_PLOT_FILE, bbox_inches='tight')
        plt.close()

    def plot_pos_distribution(self):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —á–∞—Å—Ç–µ–π —Ä–µ—á–∏"""
        vocab = self.vocabulary_analysis()
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        pos_data = {
            'user': dict(vocab['user']['pos_stats']),
            'friend': dict(vocab['friend']['pos_stats'])
        }
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —á–∞—Å—Ç–∏ —Ä–µ—á–∏
        all_pos = set(pos_data['user'].keys()).union(set(pos_data['friend'].keys()))
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —á–∏—Ç–∞–µ–º—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
        readable_pos = []
        for pos in all_pos:
            readable_pos.append(self.pos_mapping.get(pos, pos))
        
        # –ó–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —É—á–∞—Å—Ç–Ω–∏–∫–∞
        user_counts = [pos_data['user'].get(pos, 0) for pos in all_pos]
        friend_counts = [pos_data['friend'].get(pos, 0) for pos in all_pos]
        
        # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞
        x = np.arange(len(readable_pos))
        width = 0.35
        
        plt.figure(figsize=(14, 7))
        plt.bar(x - width/2, user_counts, width, label=USER_ALIAS, color=PLOT_COLORS['user'])
        plt.bar(x + width/2, friend_counts, width, label=FRIEND_ALIAS, color=PLOT_COLORS['friend'])
        
        plt.xlabel('–ß–∞—Å—Ç—å —Ä–µ—á–∏', fontsize=12)
        plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ', fontsize=12)
        plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∞—Å—Ç–µ–π —Ä–µ—á–∏', fontsize=14)
        plt.xticks(x, readable_pos, rotation=45, ha='right')
        plt.legend(fontsize=12)
        plt.grid(True, linestyle='--', alpha=0.3)
        plt.tight_layout()
        plt.savefig(POS_PLOT_FILE, bbox_inches='tight', dpi=150)
        plt.close()

    def generate_report(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        print('–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏...')
        stats = self.basic_stats()

        print('–ê–Ω–∞–ª–∏–∑ —Å–ª–æ–≤–∞—Ä–Ω–æ–≥–æ –∑–∞–ø–∞—Å–∞...')
        vocab = self.vocabulary_analysis()

        print('–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑...')
        emotion = self.emotional_analysis()
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        report = [
            "="*60,
            f"üìä –ö–û–ú–ü–õ–ï–ö–°–ù–´–ô –ê–ù–ê–õ–ò–ó –ü–ï–†–ï–ü–ò–°–ö–ò".center(60),
            f"({datetime.now().strftime('%d.%m.%Y %H:%M')})".center(60),
            "="*60,
            "\nüìå –û–°–ù–û–í–ù–´–ï –ü–û–ö–ê–ó–ê–¢–ï–õ–ò",
            "-"*40,
            f"‚ñ™ –í—Å–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π: {stats['total']}",
            f"‚ñ™ –°–æ–æ–±—â–µ–Ω–∏–π {USER_ALIAS}: {stats['user']} ({stats['user_ratio']:.1%})",
            f"‚ñ™ –°–æ–æ–±—â–µ–Ω–∏–π {FRIEND_ALIAS}: {stats['total'] - stats['user']} ({(1 - stats['user_ratio']):.1%})",
            f"\n‚ñ™ –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏—è:",
            f"  ‚ñ∏ {USER_ALIAS}: {stats['avg_length']['user']:.1f} —Å–∏–º–≤–æ–ª–æ–≤",
            f"  ‚ñ∏ {FRIEND_ALIAS}: {stats['avg_length']['friend']:.1f} —Å–∏–º–≤–æ–ª–æ–≤",
            f"\n‚ñ™ –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏–µ:",
            f"  ‚ñ∏ {USER_ALIAS}: {stats['avg_words']['user']:.1f} —Å–ª–æ–≤",
            f"  ‚ñ∏ {FRIEND_ALIAS}: {stats['avg_words']['friend']:.1f} —Å–ª–æ–≤",
            
            "\n\nüìö –õ–ï–ö–°–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó",
            "-"*40,
            f"‚ñ™ –í—Å–µ–≥–æ —Å–ª–æ–≤ (–±–µ–∑ —Å—Ç–æ–ø-—Å–ª–æ–≤): {vocab['all']['total']}",
            f"‚ñ™ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {vocab['all']['unique']}",
            f"‚ñ™ –õ–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ (TTR): {vocab['all']['diversity']:.1%}",
            f"\n‚ñ™ –°–ª–æ–≤–∞—Ä–Ω—ã–π –∑–∞–ø–∞—Å:",
            f"  ‚ñ∏ {USER_ALIAS}: {vocab['user']['unique']} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤",
            f"  ‚ñ∏ {FRIEND_ALIAS}: {vocab['friend']['unique']} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤",
            
            f"\n\nüèÜ –¢–û–ü-{TOP_WORDS_LIMIT} –°–õ–û–í",
            "-"*40,
            f"‚ñ™ {USER_ALIAS}:",
            "  " + ", ".join(f"{w}({c})" for w, c in vocab['user']['top']),
            f"\n‚ñ™ {FRIEND_ALIAS}:",
            "  " + ", ".join(f"{w}({c})" for w, c in vocab['friend']['top']),
                        
            "\n\nüé≠ –≠–ú–û–¶–ò–û–ù–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó",
            "-"*40,
            emotion,
            
            "\n" + "="*60
        ]
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        with open(OUTPUT_REPORT, 'w', encoding='utf-8') as f:
            f.write("\n".join(report))
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
        self.generate_wordcloud()
        self.plot_message_lengths()
        self.plot_pos_distribution()

# ======================
# –ó–ê–ü–£–°–ö –ê–ù–ê–õ–ò–ó–ê
# ======================
if __name__ == "__main__":
    print("="*60)
    print("üîç –ê–ù–ê–õ–ò–ó –ü–ï–†–ï–ü–ò–°–ö–ò".center(60))
    print("="*60)
    
    try:
        analyzer = ChatAnalyzer()
        
        print("\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        analyzer.load_chat()
        
        print("üìä –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏...")
        stats = analyzer.basic_stats()
        print(f"‚ñ™ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–π: {stats['total']}")
        print(f"‚ñ™ –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ {USER_ALIAS}/{FRIEND_ALIAS}: {stats['user_ratio']:.1%}")
        
        print("\nüìö –ê–Ω–∞–ª–∏–∑ —Å–ª–æ–≤–∞—Ä–Ω–æ–≥–æ –∑–∞–ø–∞—Å–∞...")
        vocab = analyzer.vocabulary_analysis()
        print(f"‚ñ™ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {vocab['all']['unique']}")
        print(f"‚ñ™ –õ–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ: {vocab['all']['diversity']:.1%}")
        
        print("\nüé≠ –ê–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–æ–Ω–∞...")
        print(analyzer.emotional_analysis().split('\n')[0])
        
        print("\nüñºÔ∏è –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π...")
        analyzer.generate_report()
        
        print("\n‚úÖ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù".center(60))
        print("="*60)
        print(f"\n–°–æ–∑–¥–∞–Ω—ã —Ñ–∞–π–ª—ã:")
        print(f"- –¢–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç: {OUTPUT_REPORT}")
        print(f"- –û–±–ª–∞–∫–æ —Å–ª–æ–≤: {WORDCLOUD_FILE}")
        print(f"- –ì—Ä–∞—Ñ–∏–∫ –¥–ª–∏–Ω—ã —Å–æ–æ–±—â–µ–Ω–∏–π: {LENGTH_PLOT_FILE}")
        print(f"- –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∞—Å—Ç–µ–π —Ä–µ—á–∏: {POS_PLOT_FILE}")
        
    except Exception as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê: {str(e)}")
        print("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–∞ –ø–µ—Ä–µ–ø–∏—Å–∫–∏ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏")